<!DOCTYPE html>
<html>

<head>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
        }

        h1,
        h2,
        h3,
        h4,
        h5,
        h6 {
            font-weight: bold;
            margin-top: 1.5em;
            margin-bottom: 0.5em;
        }

        h1 {
            font-size: 2em;
        }

        h2 {
            font-size: 1.5em;
        }

        h3 {
            font-size: 1.3em;
        }

        h4 {
            font-size: 1.1em;
        }

        a {
            color: #1a0dab;
            text-decoration: none;
        }

        a:hover {
            text-decoration: underline;
        }

        strong {
            font-weight: bold;
        }

        em {
            font-style: italic;
        }

        img {
            max-width: 100%;
            height: auto;
        }

        iframe {
            max-width: 100%;
            height: auto;
        }

        a.tip {
            border-bottom: 1px dashed;
            text-decoration: none
        }

        a.tip:hover {
            cursor: help;
            position: relative
        }

        a.tip span {
            display: none
        }

        a.tip:hover span {
            opacity: 0.95;
            border: #c0c0c0 1px dotted;
            padding: 5px 5px 5px 5px;
            display: block;
            z-index: 100;
            background: #FAF9F6;
            left: -50px;
            margin: 10px;
            width: 300px;
            position: absolute;
            top: 10px;
            text-decoration: none;
            font-size: small;

        }
    </style>
</head>

<body>
    <div>
        <h1>ECIR 2024: Exploring Emerging Trends in Information Retrieval and Natural Language Processing</h1><br><br>
        The ECIR 2024 conference is set to take place in Glasgow, Scotland from March 24th to March 28th, 2024. This
        conference will bring together researchers and experts from various domains to explore the emerging trends in
        information retrieval and natural language processing. The conference will focus on enhancing the research
        experience through advanced AI and user-centric tools, with a particular emphasis on search efficiency,
        inclusivity, and interdisciplinary collaboration. The conference will feature presentations and discussions on
        topics such as multi-modal data integration, quantum annealing, and innovative machine learning
        techniques.<br><br>
        <div>
            <figure><iframe allow="fullscreen" title="VOSViewer" class="jss1373"
                    style="align:center; width: 100%; height: 300px;"
                    src="https://vos.zeta-alpha.com/?json=https://zetaalphavector.github.io/sumblogger/ecir2024/vos/all.json"
                    data-dashlane-frameid="25634"></iframe>
                <figcaption style="font-size: 0.8em; text-align: center;">VosViewer visualization of the conference
                </figcaption>
            </figure>
        </div><br><br>
        <h2>AI-Driven Search</h2><br>Emerging trends in information retrieval and natural language processing are
        driving innovations in search efficiency, inclusivity, and interdisciplinary collaboration, with a focus on
        enhancing the research experience through advanced AI and user-centric tools.<br>
        <div>
            <figure><iframe allow="fullscreen" title="VOSViewer" class="jss1373"
                    style="align:center; width: 100%; height: 300px;"
                    src="https://vos.zeta-alpha.com/?json=https://zetaalphavector.github.io/sumblogger/ecir2024/vos/1.json"
                    data-dashlane-frameid="25634"></iframe>
                <figcaption style="font-size: 0.8em; text-align: center;">The 10 red nodes were picked as
                    representatives of the cluster</figcaption>
            </figure>
        </div><br><br>VADIS <a class="tip" target=”_blank”
            href="https://search.zeta-alpha.com/?doc_ids=014481e04bb92d9b6a1a5c55aceccb564cbf7d1a">[1]<span><b>VADIS – A
                    Variable Detection, Interlinking and Summarization System</b><br><br>VADIS revolutionizes social
                science research by interlinking survey variables with corresponding data and publications, enabling
                contextualized searches and usage.</span></a> and Scispace Literature Review <a class="tip"
            target=”_blank”
            href="https://search.zeta-alpha.com/?doc_ids=8c8b964f76f6320227dabcc37b4cdca1920fef65">[4]<span><b>SciSpace
                    Literature Review: Harnessing AI for Effortless Scientific Discovery</b><br><br>Scispace Literature
                Review revolutionizes literature exploration with AI-driven search, multilingual support, and tailored
                insights, significantly enhancing academic research efficiency.</span></a> are transforming social
        science and academic research by linking data with publications and providing AI-driven, multilingual literature
        search capabilities, respectively. IR4U2 <a class="tip" target=”_blank”
            href="https://search.zeta-alpha.com/?doc_ids=2e10c93a8c2f3843d221acc5aa03677f7ed32cfd">[2]<span><b>1
                    $$^{st}$$ Workshop on Information Retrieval for Understudied Users (IR4U2)</b><br><br>IR4U2
                champions inclusive Information Retrieval advancements, spotlighting and addressing the unique needs of
                diverse, traditionally marginalized user groups.</span></a> is pioneering inclusive Information
        Retrieval by focusing on the needs of diverse user groups, while ECIR 2024 promotes collaboration through
        workshops on academic search and bibliometrics <a class="tip" target=”_blank”
            href="https://search.zeta-alpha.com/?doc_ids=5a62500fdd3468417cfb1d442735d209c0cc3c65">[3]<span><b>Bibliometric-Enhanced
                    Information Retrieval: 14th International BIR Workshop (BIR 2024)</b><br><br>ECIR 2024's full-day
                BIR workshop will convene experts in academic search, recommendation systems, and bibliometrics,
                fostering interdisciplinary collaboration in scientometrics and NLP.</span></a> and on cooperative
        search engine development <a class="tip" target=”_blank”
            href="https://search.zeta-alpha.com/?doc_ids=fa916aff252d54c23ca227c41f6fbd7377675664">[5]<span><b>The First
                    International Workshop on Open Web Search (WOWS)</b><br><br>ECIR 2024's inaugural WOWS workshop
                invites submissions on cooperative search engine development and practical evaluation via TIREx,
                fostering innovation in tailored search solutions.</span></a>. MathMex <a class="tip" target=”_blank”
            href="https://search.zeta-alpha.com/?doc_ids=de279a22ed9965261be09e32415c88fb057dd57d">[6]<span><b>MathMex:
                    Search Engine for Math Definitions</b><br><br>MathMex revolutionizes mathematical research with an
                open-source engine leveraging SciBERT and Sentence-BERT for multifaceted definition retrieval from
                texts, images, and videos.</span></a> and the toolkit mentioned in <a class="tip" target=”_blank”
            href="https://search.zeta-alpha.com/?doc_ids=3cef7118f1dd42175143ff586375e8b2e3da4d7a">[7]<span><b>eval-rationales:
                    An End-to-End Toolkit to Explain and Evaluate Transformers-Based Models</b><br><br>Advancements in
                NLP and IR transformer model interpretability are integrated into a user-friendly toolkit for robust
                evaluation of decision rationale quality.</span></a> are advancing mathematical research and NLP model
        interpretability, respectively. LongEval Lab <a class="tip" target=”_blank”
            href="https://search.zeta-alpha.com/?doc_ids=4ab25988371098b7d59d0efef93961c2dc543ec5">[8]<span><b>LongEval:
                    Longitudinal Evaluation of Model Performance at CLEF 2024</b><br><br>LongEval Lab at CLEF 2024
                targets temporal effectiveness in IR and text classification, focusing on model resilience to data
                aging.</span></a> emphasizes the importance of model resilience over time, SUD.DL <a class="tip"
            target=”_blank”
            href="https://search.zeta-alpha.com/?doc_ids=8526dfedef64b8ee05cd846157694f429f142155">[9]<span><b>Building
                    and Evaluating a WebApp for Effortless Deep Learning Model Deployment</b><br><br>SUD.DL
                revolutionizes NLP model deployment, offering a web application that enhances efficiency, functionality,
                and discoverability for streamlined public testing.</span></a> streamlines NLP model deployment, and
        recent research on Transformer-Encoder LMs <a class="tip" target=”_blank”
            href="https://search.zeta-alpha.com/?doc_ids=99456cc0800015a07b9d9d0fa3e4062f9b7ca6e4">[10]<span><b>Investigating
                    the Usage of Formulae in Mathematical Answer Retrieval</b><br><br>Exploring Transformer-Encoder LMs
                for Mathematical Answer Retrieval, researchers found variable overlap key, identified a detrimental
                shortcut, and enhanced model accuracy by its removal.</span></a> improves mathematical answer retrieval
        by addressing model shortcuts.<br><br><br>
        <h2>Fair Personalization</h2><br>Spanning IR systems, recommendation engines, and search algorithms, recent
        research converges on enhancing user experience through fairness, personalization, and bias mitigation, while
        maintaining robust performance and utility.<br>
        <div>
            <figure><iframe allow="fullscreen" title="VOSViewer" class="jss1373"
                    style="align:center; width: 100%; height: 300px;"
                    src="https://vos.zeta-alpha.com/?json=https://zetaalphavector.github.io/sumblogger/ecir2024/vos/2.json"
                    data-dashlane-frameid="25634"></iframe>
                <figcaption style="font-size: 0.8em; text-align: center;">The 10 red nodes were picked as
                    representatives of the cluster</figcaption>
            </figure>
        </div><br><br>A tutorial provides IR experts with advanced skills in query performance prediction, extending to
        conversational search <a class="tip" target=”_blank”
            href="https://search.zeta-alpha.com/?doc_ids=0c77237754e8a43b8818868273483f2e08059822">[2]<span><b>Query
                    Performance Prediction: From Fundamentals to Advanced Techniques</b><br><br>Harnessing recent
                advancements, this tutorial equips IR experts with cutting-edge skills in query performance prediction,
                expanding into conversational search and bridging theoretical-practical divides.</span></a>, and a
        two-stage cascading retrieval pipeline is developed for sensitive content search <a class="tip" target=”_blank”
            href="https://search.zeta-alpha.com/?doc_ids=13337cdd00ef6d01344a95502a6e6e817e13710d">[3]<span><b>Cascading
                    Ranking Pipelines for Sensitivity-Aware Search</b><br><br>Developing sensitivity-aware search
                engines through two-stage cascading retrieval pipelines enables safe querying of collections with
                interspersed sensitive content.</span></a>. ComSRB, a new metric, effectively measures gender bias in
        search results <a class="tip" target=”_blank”
            href="https://search.zeta-alpha.com/?doc_ids=524d59c57bda6dfaedf0002434979e0165af9158">[4]<span><b>Measuring
                    Bias in Search Results Through Retrieval List Comparison</b><br><br>Our framework introduces ComSRB,
                a novel metric for gender bias in search results, outperforming existing methods by analyzing
                query-based document skew.</span></a>, and recent studies on graph-based recommender systems expose the
        impact of edge perturbations on consumer fairness <a class="tip" target=”_blank”
            href="https://search.zeta-alpha.com/?doc_ids=6b2e33a918a2a94b09f52c57ea3e4f3db2883adc">[5]<span><b>Robustness
                    in Fairness Against Edge-Level Perturbations in GNN-Based Recommendation</b><br><br>Shifting focus
                to fairness in graph-based recommender systems, new research reveals edge perturbations
                disproportionately compromise consumer fairness, challenging current robustness evaluation
                protocols.</span></a>. The discourse on algorithmic fairness now includes equitable considerations for
        content providers and users <a class="tip" target=”_blank”
            href="https://search.zeta-alpha.com/?doc_ids=78a103d3895f269806919fb203566be68c6aa879">[6]<span><b>Shuffling
                    a Few Stalls in a Crowded Bazaar: Potential Impact of Document-Side Fairness on Unprivileged
                    Info-Seekers</b><br><br>Exploring the nuances of algorithmic fairness, recent inquiries highlight a
                shift towards balancing equity for both content providers and search engine users.</span></a>, and
        recommendation systems are being evaluated with methods that control the False Discovery Rate <a class="tip"
            target=”_blank”
            href="https://search.zeta-alpha.com/?doc_ids=81885ae4852c07a2f76a62aa1c5864117c46df74">[7]<span><b>Multiple
                    Testing for IR and Recommendation System Experiments</b><br><br>Extending beyond TREC data, this
                research evaluates recommendation systems using multiple comparison procedures that control the False
                Discovery Rate, addressing the MCP in IR experiments.</span></a>. The TALL framework counters
        collaborative filtering bias by ensembling local models <a class="tip" target=”_blank”
            href="https://search.zeta-alpha.com/?doc_ids=2796e0b6e79314790d24319f90a570ba3fcb3cf2">[8]<span><b>Countering
                    Mainstream Bias via End-to-End Adaptive Local Learning</b><br><br>Addressing mainstream bias in
                collaborative filtering, the TALL framework enhances recommendation quality by adaptively ensembling
                local models and synchronizing user learning paces.</span></a>, and GeoGrouse boosts O2O recommendations
        through geographical group-specific modeling <a class="tip" target=”_blank”
            href="https://search.zeta-alpha.com/?doc_ids=394df2b2c1ae70629851a3ac12944155e612b264">[9]<span><b>An
                    Adaptive Framework of Geographical Group-Specific Network on O2O Recommendation</b><br><br>GeoGrouse
                enhances O2O recommendation by leveraging geographical group-specific modeling and an automatic grouping
                paradigm, significantly improving business outcomes through personalized user preference
                analysis.</span></a>.<br><br><br>
        <h2>Multi-Modal Integration</h2><br>Spanning diverse domains, recent advancements underscore a trend towards
        integrating multi-modal data and novel machine learning techniques to enhance detection, decision-making, and
        information retrieval across digital platforms.<br>
        <div>
            <figure><iframe allow="fullscreen" title="VOSViewer" class="jss1373"
                    style="align:center; width: 100%; height: 300px;"
                    src="https://vos.zeta-alpha.com/?json=https://zetaalphavector.github.io/sumblogger/ecir2024/vos/3.json"
                    data-dashlane-frameid="25634"></iframe>
                <figcaption style="font-size: 0.8em; text-align: center;">The 10 red nodes were picked as
                    representatives of the cluster</figcaption>
            </figure>
        </div><br><br>Affiliate marketing strategies are found to degrade search engine quality through pervasive link
        spam and subpar content <a class="tip" target=”_blank”
            href="https://search.zeta-alpha.com/?doc_ids=9a93cd69414a3b61d57c352a642a2ddefa60a5d8">[1]<span><b>Is Google
                    Getting Worse? A Longitudinal Investigation of SEO Spam in Search Engines</b><br><br>Exploratory
                research reveals that affiliate marketing strategies are compromising search engine quality, with
                prevalent low-quality content and link spam undermining user experience.</span></a>, while BioASQ's
        twelfth challenge <a class="tip" target=”_blank”
            href="https://search.zeta-alpha.com/?doc_ids=0e86bb799d0cbd1ad095254951bf7b76849dd417">[2]<span><b>BioASQ
                    at CLEF2024: The Twelfth Edition of the Large-Scale Biomedical Semantic Indexing and Question
                    Answering Challenge</b><br><br>BioASQ's twelfth challenge elevates biomedical information access by
                benchmarking novel semantic indexing and question-answering methods across multilingual
                tasks.</span></a> and iDPP@CLEF <a class="tip" target=”_blank”
            href="https://search.zeta-alpha.com/?doc_ids=625c2eab9f0a6148a0e12f335f2536bb7b0f3785">[3]<span><b>iDPP@CLEF
                    2024: The Intelligent Disease Progression Prediction Challenge</b><br><br>Exploring ALS and MS
                progression, iDPP@CLEF integrates retrospective and prospective patient data with environmental inputs
                to enhance clinical decision-making and intervention timeliness.</span></a> push the boundaries of
        biomedical information retrieval and patient data analysis, respectively. The IR-MMCSG system <a class="tip"
            target=”_blank”
            href="https://search.zeta-alpha.com/?doc_ids=7169ccd1408c6b39e6409bc455b005a81f687c56">[4]<span><b>Yes, This
                    Is What I Was Looking For! Towards Multi-modal Medical Consultation Concern Summary
                    Generation</b><br><br>Leveraging multi-modal cues and personal context, a novel IR-MMCSG system
                enhances medical concern summary generation from patient-doctor consultations.</span></a> and eRisk <a
            class="tip" target=”_blank”
            href="https://search.zeta-alpha.com/?doc_ids=89677c52697709be80d50de51bbcdf2f3357b7e3">[5]<span><b>eRisk
                    2024: Depression, Anorexia, and Eating Disorder Challenges</b><br><br>Launched in 2017, eRisk has
                advanced early Internet risk detection, developing models and datasets for mental health issues, with
                updates planned for 2024.</span></a> both contribute to medical informatics by improving consultation
        summaries and early risk detection. Advances in NLP for ethical applications are marked by strides in bias
        detection and debiasing <a class="tip" target=”_blank”
            href="https://search.zeta-alpha.com/?doc_ids=19c238ffa89a89c76e38d42249d0586d1a727a23">[6]<span><b>Bias
                    Detection and Mitigation in Textual Data: A Study on Fake News and Hate Speech
                    Detection</b><br><br>Exploring bias detection models and debiasing methods enhances fake news and
                hate speech identification, fostering fairness and ethical NLP applications.</span></a>, as well as the
        high-accuracy MFVIEW model for fake news identification <a class="tip" target=”_blank”
            href="https://search.zeta-alpha.com/?doc_ids=1d3b7f1c68bd0781ada927e108789bd8a51e3b25">[7]<span><b>MFVIEW:
                    Multi-modal Fake News Detection with View-Specific Information Extraction</b><br><br>MFVIEW, a novel
                model, enhances fake news detection by integrating multi-modal and view-specific information, achieving
                over 90% accuracy on Twitter and Weibo datasets.</span></a>. CheckThat! 2023 <a class="tip"
            target=”_blank”
            href="https://search.zeta-alpha.com/?doc_ids=312fef01fe59695e21ad6de8445baadee479b3a2">[8]<span><b>The
                    CLEF-2024 CheckThat! Lab: Check-Worthiness, Subjectivity, Persuasion, Roles, Authorities,
                    and Adversarial Robustness</b><br><br>Expanding its scope, CheckThat! 2023 introduces six
                multilingual tasks, including novel challenges in rumor verification and credibility assessment
                robustness.</span></a> broadens its remit with new multilingual tasks, while novel models significantly
        enhance depression detection <a class="tip" target=”_blank”
            href="https://search.zeta-alpha.com/?doc_ids=223f4f2eb836ace12b5d90d562976fc5a44b5d5b">[9]<span><b>Reading
                    Between the Frames: Multi-modal Depression Detection in Videos from Non-verbal
                    Cues</b><br><br>Leveraging a novel multi-modal temporal model, researchers significantly improved
                depression detection in real-world videos by integrating diverse non-verbal cues, outperforming
                benchmarks.</span></a> and sarcasm discernment in memes <a class="tip" target=”_blank”
            href="https://search.zeta-alpha.com/?doc_ids=e61a479080665ceae597b528c47ef33d11c6cdc4">[10]<span><b>Mu2STS:
                    A Multitask Multimodal Sarcasm-Humor-Differential Teacher-Student Model for Sarcastic Meme
                    Detection</b><br><br>Mu2STS, a novel deep learning model, adeptly distinguishes sarcasm from humor
                in memes, outshining existing models in empirical evaluations on the pioneering SHMH
                dataset.</span></a>.<br><br><br>
        <h2>Retrieval Optimization</h2><br>Advancements in retrieval systems and data classification are marked by
        innovative methods that enhance performance and efficiency, reflecting a trend towards optimizing noisy data
        extraction, contextual understanding, and domain-specific adaptation.<br>
        <div>
            <figure><iframe allow="fullscreen" title="VOSViewer" class="jss1373"
                    style="align:center; width: 100%; height: 300px;"
                    src="https://vos.zeta-alpha.com/?json=https://zetaalphavector.github.io/sumblogger/ecir2024/vos/4.json"
                    data-dashlane-frameid="25634"></iframe>
                <figcaption style="font-size: 0.8em; text-align: center;">The 10 red nodes were picked as
                    representatives of the cluster</figcaption>
            </figure>
        </div><br><br>Our binary and adaptive feature weighting method excels in noisy data classification <a
            class="tip" target=”_blank”
            href="https://search.zeta-alpha.com/?doc_ids=2fc88a32c65c778f5aa66ddea93bd17fc4ccda95">[1]<span><b>An
                    Adaptive Feature Selection Method for Learning-to-Enumerate Problem</b><br><br>Harnessing a binary
                and adaptive feature weighting approach, our method efficiently extracts and classifies target instances
                from noisy datasets, outperforming existing techniques.</span></a>, while contextualized neural
        embeddings elevate our supervised QPP method, as evidenced on MS MARCO V1 <a class="tip" target=”_blank”
            href="https://search.zeta-alpha.com/?doc_ids=4c9e694a30c6a5587196e5b7b38957ed0f9109bb">[2]<span><b>BertPE: A
                    BERT-Based Pre-retrieval Estimator for Query Performance Prediction</b><br><br>Employing
                contextualized neural embeddings, our supervised QPP method significantly outperforms existing
                pre-retrieval models, validated on MS MARCO V1 with synthetic relevance judgments.</span></a>.
        ImageCLEF's 2024 benchmarks highlight a significant rise in multimodal data retrieval tasks <a class="tip"
            target=”_blank”
            href="https://search.zeta-alpha.com/?doc_ids=5149b827eba7c982f2b26f4a6f54c83b312f5f3b">[3]<span><b>Advancing
                    Multimedia Retrieval in Medical, Social Media and Content Recommendation Applications with ImageCLEF
                    2024</b><br><br>For over two decades, ImageCLEF has benchmarked multimodal data retrieval, with
                ImageCLEF 2024 focusing on medical AI, argumentation, and cultural heritage tasks, showing a 67%
                participation surge.</span></a>. In text retrieval, shallow transformer models, such as TinyBERT-gBCE,
        demonstrate remarkable efficiency gains <a class="tip" target=”_blank”
            href="https://search.zeta-alpha.com/?doc_ids=10c509c6611a7337b978fe9eb2d0b5c0357051da">[4]<span><b>Shallow
                    Cross-Encoders for Low-Latency Retrieval</b><br><br>Shallow transformer models outperform full-scale
                counterparts in low-latency text retrieval, with TinyBERT-gBCE showing a 51% NDCG@10 gain over
                MonoBERT-Large.</span></a>, and VEMO unifies cross-modal search tasks with fewer network parameters <a
            class="tip" target=”_blank”
            href="https://search.zeta-alpha.com/?doc_ids=226709a2c3b01724d447f97d5cf8706e161e5d5f">[5]<span><b>VEMO: A
                    Versatile Elastic Multi-modal Model for Search-Oriented Multi-task Learning</b><br><br>Introducing
                VEMO, a novel multi-task learning model, adeptly unifying cross-modal search, entity recognition, and
                text spotting, achieving superior performance with reduced network parameters.</span></a>.
        Multi-positive contrastive learning bolsters dense retrieval against typos <a class="tip" target=”_blank”
            href="https://search.zeta-alpha.com/?doc_ids=22a6d28be2579c60eb0c792f137af8ce239b65f3">[6]<span><b>Improving
                    the Robustness of Dense Retrievers Against Typos via Multi-Positive Contrastive
                    Learning</b><br><br>Dense retrieval's robustness to typos is enhanced by employing multi-positive
                contrastive learning, utilizing all typoed variants, yielding improved retrieval
                performance.</span></a>, and corpus-specific pre-training of BERT improves sparse retrieval systems <a
            class="tip" target=”_blank”
            href="https://search.zeta-alpha.com/?doc_ids=33111c81b86e4674805cd0face96511726ba209b">[7]<span><b>Improved
                    Learned Sparse Retrieval with Corpus-Specific Vocabularies</b><br><br>Leveraging corpus-specific
                vocabularies and pre-training BERT on target corpora significantly enhances sparse retrieval systems'
                efficiency and effectiveness by up to 12%.</span></a>. Adapting language techniques for pre-training
        sparse retrievers <a class="tip" target=”_blank”
            href="https://search.zeta-alpha.com/?doc_ids=66f84376887204a5d7f38c1cf9f29bcdf8fee0a4">[8]<span><b>Simple
                    Domain Adaptation for Sparse Retrievers</b><br><br>Transposing language adaptation techniques to
                pre-train sparse first-stage retrievers enhances domain-specific performance without annotated
                data.</span></a> and a novel negative sample selection method <a class="tip" target=”_blank”
            href="https://search.zeta-alpha.com/?doc_ids=b2b37e5f4efd5d048830f6eb10490ef316063080">[9]<span><b>InDi:
                    Informative and Diverse Sampling for Dense Retrieval</b><br><br>Implementing our novel negative
                sample selection method, which emphasizes informativeness and diversity, significantly enhances dense
                retrieval models, yielding measurable performance gains with minimal overhead.</span></a> both
        significantly boost retrieval model performance. Lastly, a new dataset and Transformer-based method advance the
        dating of cultural heritage photos <a class="tip" target=”_blank”
            href="https://search.zeta-alpha.com/?doc_ids=de562a13e099a3a28e99380190a59f90c724e958">[10]<span><b>A
                    Transformer-Based Object-Centric Approach for Date Estimation of Historical
                    Photographs</b><br><br>Introducing a novel dataset and a Transformer-based approach, researchers
                significantly enhance cultural heritage photo dating, outperforming prior methods and offering public
                access to resources.</span></a>.<br><br><br>
        <h2>Neural Summarization</h2><br>Spanning innovative summarization techniques to advanced information retrieval,
        these documents collectively underscore a trend towards integrating sophisticated neural architectures and
        user-centric designs to enhance the efficiency and accuracy of data processing across various domains.<br>
        <div>
            <figure><iframe allow="fullscreen" title="VOSViewer" class="jss1373"
                    style="align:center; width: 100%; height: 300px;"
                    src="https://vos.zeta-alpha.com/?json=https://zetaalphavector.github.io/sumblogger/ecir2024/vos/5.json"
                    data-dashlane-frameid="25634"></iframe>
                <figcaption style="font-size: 0.8em; text-align: center;">The 10 red nodes were picked as
                    representatives of the cluster</figcaption>
            </figure>
        </div><br><br>Our research presents a novel extractive summarization technique combining a GNN encoder with an
        RNN decoder, complemented by an interactive interface <a class="tip" target=”_blank”
            href="https://search.zeta-alpha.com/?doc_ids=1b1e6b28b87577649f802ccbc2ee627b23f5fbc5">[1]<span><b>Interactive
                    Document Summarization</b><br><br>Unveiling an innovative extractive summarization technique, our
                work integrates a GNN encoder with an RNN decoder, enhanced by an interactive user
                interface.</span></a>, while the ALTARS workshop focuses on refining High-recall IR systems' test
        collections <a class="tip" target=”_blank”
            href="https://search.zeta-alpha.com/?doc_ids=1b80eb45f81a8a4f82d06c728f67ba840fb356e9">[2]<span><b>Third
                    Workshop on Augmented Intelligence in Technology-Assisted Review Systems (ALTARS)</b><br><br>ALTARS
                workshop's third edition zeroes in on developing test collections for High-recall IR systems, aiming to
                refine evaluation guidelines for comprehensive document retrieval.</span></a>. A hierarchical
        information system has been shown to improve sensitivity review <a class="tip" target=”_blank”
            href="https://search.zeta-alpha.com/?doc_ids=061748a46aa998a696652567412ca308ce769023">[3]<span><b>Displaying
                    Evolving Events Via Hierarchical Information Threads for Sensitivity Review</b><br><br>Introducing
                an innovative system that enhances sensitivity review efficiency by organizing information
                hierarchically, our user study confirms its speed and accuracy benefits over conventional
                methods.</span></a>, and a DQN-based online crisis timeline generation method demonstrates superior
        performance in handling data redundancy <a class="tip" target=”_blank”
            href="https://search.zeta-alpha.com/?doc_ids=b128e7c1b63cfe41cb60c01118aede8f5818d9ef">[4]<span><b>DQNC2S:
                    DQN-Based Cross-Stream Crisis Event Summarizer</b><br><br>An online crisis timeline generation
                method using DQNs outperforms existing models on CrisisFACTS 2022 by efficiently handling data
                redundancy and scalability.</span></a>. Hierarchical Text Classification is re-envisioned as a
        generative task, prompting a reevaluation of modeling choices <a class="tip" target=”_blank”
            href="https://search.zeta-alpha.com/?doc_ids=25a6786a977a7cec7fdd0c77de381a50019a8d60">[5]<span><b>A Study
                    on Hierarchical Text Classification as a Seq2seq Task</b><br><br>Advancements in generative neural
                models have transformed Hierarchical Text Classification into a generative task, prompting an analysis
                of modeling choices and their impacts, supported by an open framework for future research.</span></a>,
        and CE_FS emerges as a leading method for legal answer retrieval <a class="tip" target=”_blank”
            href="https://search.zeta-alpha.com/?doc_ids=b018e498d6e76e67fcec10e47be1501e3568741a">[6]<span><b>Answer
                    Retrieval in Legal Community Question Answering</b><br><br>CE_FS, a cross-encoder re-ranker
                utilizing fine-grained structured inputs, enhances legal answer retrieval, outperforming others on the
                new LegalQA benchmark dataset.</span></a>. ARElight offers a modular pipeline for segmenting and
        extracting information from large documents <a class="tip" target=”_blank”
            href="https://search.zeta-alpha.com/?doc_ids=e87dc5eae4c98fbc597488dc600d67b6e311fd5e">[7]<span><b>ARElight:
                    Context Sampling of Large Texts for Deep Learning Relation Extraction</b><br><br>ARElight
                efficiently segments and extracts information from large documents, enhancing NLP with a modular
                pipeline for diverse, structured text analysis applications.</span></a>, and zero-shot large language
        models with calibration show promise for systematic review screening <a class="tip" target=”_blank”
            href="https://search.zeta-alpha.com/?doc_ids=f892b9696fc1c1169c5618b1b3edc9187b6db1f2">[8]<span><b>Zero-Shot
                    Generative Large Language Models for Systematic Review Screening Automation</b><br><br>Exploring
                zero-shot large language models with calibration for systematic review screening, this research reveals
                time-saving potential and targeted recall achievement.</span></a>. Text2Story has been advancing
        narrative extraction since 2018 <a class="tip" target=”_blank”
            href="https://search.zeta-alpha.com/?doc_ids=34abef05eba0dcb66279a9d8fd2dfa08bdef84d3">[9]<span><b>The 7th
                    International Workshop on Narrative Extraction from Texts: Text2Story 2024</b><br><br>Since 2018,
                Text2Story has fostered advances in narrative extraction from texts, grappling with narrative structure
                representation and integration into AI frameworks like transformers.</span></a>, and a workshop explores
        the extraction of geographic information from text, highlighting its applications <a class="tip" target=”_blank”
            href="https://search.zeta-alpha.com/?doc_ids=f8118eae91a99acc8a23a698c5ef62865ef18f93">[10]<span><b>2nd
                    International Workshop on Geographic Information Extraction from Texts (GeoExT
                    2024)</b><br><br>Exploring the extraction of geographic information from text, this workshop
                addresses breakthroughs and challenges in retrieval, disaster response, and spatial
                studies.</span></a>.<br><br><br>
        <h2>Recommender Innovations</h2><br>Advancements in recommender systems are converging on sophisticated machine
        learning techniques, emphasizing efficiency, multimodality, and domain adaptability to enhance user experience
        and precision.<br>
        <div>
            <figure><iframe allow="fullscreen" title="VOSViewer" class="jss1373"
                    style="align:center; width: 100%; height: 300px;"
                    src="https://vos.zeta-alpha.com/?json=https://zetaalphavector.github.io/sumblogger/ecir2024/vos/6.json"
                    data-dashlane-frameid="25634"></iframe>
                <figcaption style="font-size: 0.8em; text-align: center;">The 10 red nodes were picked as
                    representatives of the cluster</figcaption>
            </figure>
        </div><br><br>The KGCCL model <a class="tip" target=”_blank”
            href="https://search.zeta-alpha.com/?doc_ids=2249a201309ae1ac078d585a222776f957ee8c1e">[1]<span><b>Knowledge
                    Graph Cross-View Contrastive Learning for Recommendation</b><br><br>Leveraging contrastive learning
                and noise augmentation, the KGCCL model adeptly mitigates supervision sparsity and information loss,
                outshining state-of-the-art methods in recommendation systems.</span></a> excels in recommendation
        systems by using contrastive learning to address supervision sparsity, while the GLAD model <a class="tip"
            target=”_blank”
            href="https://search.zeta-alpha.com/?doc_ids=230aaa44adb33c9debc5c02c24b6284648068e94">[2]<span><b>GLAD:
                    Graph-Based Long-Term Attentive Dynamic Memory for Sequential Recommendation</b><br><br>Harnessing a
                novel transformer-based GLAD model with dynamic, graph-external memory, we enhance e-commerce
                recommender systems, balancing performance with computational efficiency.</span></a> and Transformer
        architectures <a class="tip" target=”_blank”
            href="https://search.zeta-alpha.com/?doc_ids=2f1db3b126169f4463908443719848fa3122e466">[3]<span><b>Transformers
                    for Sequential Recommendation</b><br><br>Harnessing Transformer architectures, originally designed
                for language modeling, this tutorial addresses their adaptation and optimization challenges for
                state-of-the-art sequential recommendation systems with large item sets.</span></a> push the boundaries
        of e-commerce and sequential recommendation systems, respectively. Knowledge distillation <a class="tip"
            target=”_blank”
            href="https://search.zeta-alpha.com/?doc_ids=42e6e33ef0ba531074b77f958a50e94af1a1254c">[4]<span><b>Lightweight
                    Modality Adaptation to Sequential Recommendation via Correlation Supervision</b><br><br>Our novel
                knowledge distillation method enhances Sequential Recommenders by preserving modality information and
                improving efficiency, outperforming baselines by 6.8%.</span></a> and Neuro-Symbolic computing <a
            class="tip" target=”_blank”
            href="https://search.zeta-alpha.com/?doc_ids=79f48cec55538edc98bf3b2503a5b0f7a24da804">[5]<span><b>Mitigating
                    Data Sparsity via Neuro-Symbolic Knowledge Transfer</b><br><br>Leveraging Neuro-Symbolic computing
                and Logic Tensor Networks, our novel approach enhances recommender systems by transferring cross-domain
                knowledge, outperforming baselines even with sparse datasets.</span></a> further refine these systems,
        with the latter excelling in sparse data scenarios, which contrasts starkly with the data-rich environments <a
            class="tip" target=”_blank”
            href="https://search.zeta-alpha.com/?doc_ids=17b28bba9e498d991ffb9aad5777d284688f8dca">[6]<span><b>Knowledge
                    Transfer from Resource-Rich to Resource-Scarce Environments</b><br><br>Limited data in
                resource-scarce environments hinders user experience, contrasting with the abundant, detailed
                information in resource-rich settings.</span></a>. The MMCRec model <a class="tip" target=”_blank”
            href="https://search.zeta-alpha.com/?doc_ids=30d34bf4ae7031df53571ee0410c7e0176c69622">[7]<span><b>MMCRec:
                    Towards Multi-modal Generative AI in Conversational Recommendation</b><br><br>Harnessing text,
                images, voice, and video, the Multi-Modal Conversational Recommender System (MMCRec) model significantly
                enhances real-world recommendation performance and experience.</span></a> leverages multimodal data to
        enhance user experience, and Self-Contrastive Learning <a class="tip" target=”_blank”
            href="https://search.zeta-alpha.com/?doc_ids=78d901e32f7c09671be3bd901840dde71b66f19a">[8]<span><b>Self
                    Contrastive Learning for Session-Based Recommendation</b><br><br>Self-Contrastive Learning (SCL)
                streamlines session-based recommendation by directly optimizing item representation uniformity,
                significantly boosting model precision and interpretability without complex sample
                construction.</span></a> simplifies session-based recommendations. Meanwhile, a novel neural strategy <a
            class="tip" target=”_blank”
            href="https://search.zeta-alpha.com/?doc_ids=71c61edfbe9400204700d3d3995d04886e4d1393">[9]<span><b>A
                    Streaming Approach to Neural Team Formation Training</b><br><br>Our novel neural training strategy
                outperforms existing models in predicting expert team success by dynamically incorporating skill and
                collaboration evolution over time.</span></a> adeptly predicts team success, and a new method utilizing
        reward model outputs <a class="tip" target=”_blank”
            href="https://search.zeta-alpha.com/?doc_ids=d7a863e970ccd55f91f5f5a709c1da11f85ac8d4">[10]<span><b>Learning
                    Action Embeddings for Off-Policy Evaluation</b><br><br>Leveraging trained reward model outputs for
                action embeddings, our method enhances off-policy evaluation, outperforming MIPS and baselines in
                diverse datasets.</span></a> improves off-policy evaluation.<br><br><br>

        <h2>Conversational Efficiency</h2><br>Exploring innovative methods, researchers are enhancing information
        retrieval and conversational AI, with a focus on efficiency, accuracy, and cross-domain applicability, often
        outperforming traditional models and benchmarks.<br>
        <div>
            <figure><iframe allow="fullscreen" title="VOSViewer" class="jss1373"
                    style="align:center; width: 100%; height: 300px;"
                    src="https://vos.zeta-alpha.com/?json=https://zetaalphavector.github.io/sumblogger/ecir2024/vos/7.json"
                    data-dashlane-frameid="25634"></iframe>
                <figcaption style="font-size: 0.8em; text-align: center;">The 10 red nodes were picked as
                    representatives of the cluster</figcaption>
            </figure>
        </div><br><br>ColBERT's retrieval approach <a class="tip" target=”_blank”
            href="https://search.zeta-alpha.com/?doc_ids=18422a6080a9af1c842880b2c5bd0d03c977717b">[1]<span><b>Beneath
                    the [MASK</b><br><br>An Analysis of Structural Query Tokens in ColBERT]: ColBERT leverages token
                embeddings and cosine similarity for retrieval, with sensitivity to token order in [MASK] and [Q]
                embeddings, unlike [CLS] and [SEP].</span></a> is complemented by GenQREnsemble's ensemble-based
        prompting for query reformulation <a class="tip" target=”_blank”
            href="https://search.zeta-alpha.com/?doc_ids=693ffa727371c08830a994ff203decdcc62f7b94">[2]<span><b>GenQREnsemble:
                    Zero-Shot LLM Ensemble Prompting for Generative Query Reformulation</b><br><br>GenQREnsemble, an
                ensemble-based prompting technique for query reformulation, outperforms prior zero-shot methods,
                enhancing retrieval metrics significantly across multiple IR benchmarks.</span></a>, while a novel MMRC
        method <a class="tip" target=”_blank”
            href="https://search.zeta-alpha.com/?doc_ids=527a34518c1833ca488dac6c89625a62283420bd">[3]<span><b>Attend
                    All Options at Once: Full Context Input for Multi-choice Reading
                    Comprehension</b><br><br>Introducing a novel MMRC approach, this method enhances option relation
                capture and efficiency, outperforming on COSMOS-QA and offering cross-domain applicability.</span></a>
        and an innovative conversational search technique <a class="tip" target=”_blank”
            href="https://search.zeta-alpha.com/?doc_ids=6d562b28fcc55b430383c8730db4a1230e71e29e">[4]<span><b>Estimating
                    the Usefulness of Clarifying Questions and Answers for Conversational Search</b><br><br>Introducing
                an innovative method, our research enhances conversational search by classifying and integrating useful
                clarifying questions and answers, outperforming traditional baselines.</span></a> both demonstrate
        superior performance in their respective domains. Semantic search in oral history archives benefits from ASR and
        Transformer-based networks <a class="tip" target=”_blank”
            href="https://search.zeta-alpha.com/?doc_ids=3220acd232ac8bc3e7712e3d0b9447548789b492">[5]<span><b>Asking
                    Questions Framework for Oral History Archives</b><br><br>Leveraging ASR and Transformer-based neural
                networks, researchers developed a semantic search tool that generates and filters relevant questions for
                efficient exploration of vast oral history archives.</span></a>, and a Large Language Model adeptly
        incorporates web searches to minimize hallucinations <a class="tip" target=”_blank”
            href="https://search.zeta-alpha.com/?doc_ids=483124d555f4e021f139d154637e5062e5d72a96">[6]<span><b>Navigating
                    Uncertainty: Optimizing API Dependency for Hallucination Reduction in Closed-Book
                    QA</b><br><br>Introducing a Large Language Model that judiciously integrates web searches, our
                approach reduces hallucination and optimizes computational efficiency with a \(62\%\) API usage
                rate.</span></a>. Encoder-decoder models transform task instructions to enhance TOD systems <a
            class="tip" target=”_blank”
            href="https://search.zeta-alpha.com/?doc_ids=4dc9417cf270883986532eca743df862f8e3e875">[7]<span><b>Simulated
                    Task Oriented Dialogues for Developing Versatile Conversational Agents</b><br><br>Transforming task
                instructions into dialogues using encoder-decoder models significantly enhances TOD systems'
                performance, particularly in novel domains.</span></a>, and a sentence-level classifier in
        conversational AI predicts answerability with high accuracy <a class="tip" target=”_blank”
            href="https://search.zeta-alpha.com/?doc_ids=4e3442c36cc4a8d75e5644832987914f4861979a">[8]<span><b>Towards
                    Reliable and Factual Response Generation: Detecting Unanswerable Questions in Information-Seeking
                    Conversations</b><br><br>Employing a sentence-level classifier and aggregating predictions, our
                method accurately predicts answerability in conversational AI, outperforming state-of-the-art
                LLMs.</span></a>. Digital advancements in libraries <a class="tip" target=”_blank”
            href="https://search.zeta-alpha.com/?doc_ids=6150f1b06c749853e3c8d8f39c03c2db9b5c1090">[9]<span><b>Semantic
                    Search in Archive Collections Through Interpretable and Adaptable Relation Extraction About Person
                    and Places</b><br><br>Recent campaigns have significantly advanced the digitization of collections
                in libraries and archives, enhancing accessibility and preservation.</span></a> pair with context-guided
        question recommendation to boost in-car conversational systems <a class="tip" target=”_blank”
            href="https://search.zeta-alpha.com/?doc_ids=6e18621a54755b3ef7132cc2ffe57f467cf37836">[10]<span><b>Incorporating
                    Query Recommendation for Improving In-Car Conversational Search</b><br><br>Introducing
                context-guided question recommendation enhances in-car conversational systems, significantly improving
                document retrieval and response accuracy by 48% and 22%, respectively.</span></a>.<br><br><br>
        <h2>Quantum Information Retrieval</h2><br>Delving into the quantum realm, researchers are pioneering the
        integration of Quantum Annealing to elevate the capabilities of Information Retrieval and Recommender Systems,
        heralding a new era of computational ingenuity.<br>
        <div>
            <figure><iframe allow="fullscreen" title="VOSViewer" class="jss1373"
                    style="align:center; width: 100%; height: 300px;"
                    src="https://vos.zeta-alpha.com/?json=https://zetaalphavector.github.io/sumblogger/ecir2024/vos/8.json"
                    data-dashlane-frameid="25634"></iframe>
                <figcaption style="font-size: 0.8em; text-align: center;">The 10 red nodes were picked as
                    representatives of the cluster</figcaption>
            </figure>
        </div><br><br>Quantum Annealing (QA) is poised to revolutionize Information Retrieval and Recommender Systems by
        offering enhanced efficiency in handling large, diverse datasets <a class="tip" target=”_blank”
            href="https://search.zeta-alpha.com/?doc_ids=fa2c7fe817596610848d34b5d33ba682e5f44304">[2]<span><b>Quantum
                    Computing for Information Retrieval and Recommender Systems</b><br><br>Quantum computing promises
                enhanced efficiency in processing vast, diverse datasets for Information Retrieval and Recommender
                Systems through Quantum Annealing applications.</span></a>. The QuantumCLEF lab's inaugural tasks focus
        on assessing and innovating QA applications, thereby encouraging interdisciplinary collaboration to push the
        boundaries of current technology <a class="tip" target=”_blank”
            href="https://search.zeta-alpha.com/?doc_ids=f75fdb1ca7fcfbac0711196fc030ba37d274d82b">[1]<span><b>QuantumCLEF
                    - Quantum Computing at CLEF</b><br><br>Quantum Annealing enhances Information Retrieval and
                Recommender Systems, as QuantumCLEF lab's inaugural tasks assess and innovate QA applications, fostering
                interdisciplinary collaboration.</span></a>.<br><br> The ECIR 2024 conference promises to be an exciting
        event for researchers and experts in the field of information retrieval and natural language processing. With a
        focus on emerging trends and innovative techniques, the conference will provide a platform for interdisciplinary
        collaboration and knowledge sharing. Attendees can expect to gain insights into the latest advancements in
        search efficiency, inclusivity, and user-centric tools, and explore the potential of quantum annealing and
        multi-modal data integration.
    </div>
</body>

</html>