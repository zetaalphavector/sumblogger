<!DOCTYPE html><html><head>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
        }
        h1, h2, h3, h4, h5, h6 {
            font-weight: bold;
            margin-top: 1.5em;
            margin-bottom: 0.5em;
        }
       h1 {
            font-size: 2em;
        }
        h2 {
            font-size: 1.5em;
        }
        h3 {
            font-size: 1.3em;
        }
        h4 {
            font-size: 1.1em;
        }
        a {
            color: #1a0dab;
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
        strong {
            font-weight: bold;
        }
        em {
            font-style: italic;
        }
        img {
            max-width: 100%;
            height: auto;
        }
        iframe {
            max-width: 100%;
            height: auto;
        }
        a.tip {
            border-bottom: 1px dashed;
            text-decoration: none
        }
        a.tip:hover {
            cursor: help;
            position: relative
        }
        a.tip span {
            display: none
        }
        a.tip:hover span {
            opacity: 0.95;
            border: #c0c0c0 1px dotted;
            padding: 5px 5px 5px 5px;
            display: block;
            z-index: 100;
            background: #FAF9F6;
            left: -50px;
            margin: 10px;
            width: 300px;
            position: absolute;
            top: 10px;
            text-decoration: none;
            font-size: small;
            
        }
    </style>
    </head><body><div><h1>ICLR 2023: Exploring the Latest Trends in Machine Learning and Beyond</h1><br><br><br> <br>The International Conference on Learning Representations (ICLR) is one of the most prestigious conferences in the field of machine learning, attracting researchers and practitioners from all over the world. The conference provides a platform for sharing the latest research and developments in the field, as well as discussing the challenges and opportunities that lie ahead. <br><br>ICLR 2023 will take place in Kawai from May 11 to May 17, 2023, and promises to be an exciting event for anyone interested in machine learning and related fields. The conference will feature a wide range of topics, including reinforcement learning, natural language processing, computer vision, optimization techniques, and more. <br><br>One of the key themes of ICLR 2023 is the exploration of new trends and emerging technologies in machine learning. The conference will showcase the latest research in areas such as self-supervised learning, federated learning, graph neural networks, and adversarial attacks, among others. Attendees will have the opportunity to learn about cutting-edge techniques and approaches that are shaping the future of machine learning. <br><br>ICLR 2023 will also provide a forum for researchers to discuss the common challenges and opportunities that arise in machine learning research. From improving model robustness and generalization to addressing privacy concerns and ethical considerations, the conference will explore the many facets of machine learning and its impact on society.<br><br><br><br><br><h2>Reinforcement Learning Advances</h2><br>Recent advances in reinforcement learning (RL) have shown promising results in improving sample-efficiency, performance, and generalization of agents. These include new approaches for distilling controllers, utilizing human-written instruction manuals, and learning object interaction skills. Additionally, novel methods for pre-training and adapting agents to new tasks, as well as data-driven setups for offline multi-objective RL, have been proposed.<br><br><br>Empirical studies show that leveraging a few demonstrations can enhance the sample-efficiency of model-based RL for visuo-motor control <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=029c691f64e2b11484b6965040709ce93760110f">[1]<span><b>MoDem: Accelerating Visual Model-Based Reinforcement Learning with Demonstrations</b><br><br>Leveraging a few demonstrations can significantly improve the sample-efficiency of model-based RL for visuo-motor control, according to empirical studies.</span></a>. WAE-MDP <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=14c9e1861b0e797315fe93a03886d8065dc7f8f8">[2]<span><b>Wasserstein Auto-encoded MDPs: Formal Verification of Efficiently Distilled RL Policies with Many-sided Guarantees</b><br><br>WAE-MDP is a new approach that distills formally verifiable controllers from any RL policy, providing bisimulation guarantees and better latent model quality.</span></a> provides bisimulation guarantees and better latent model quality, while the Read and Reward framework <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=2bc827b71cc78c9343750efda319155b1186175c">[3]<span><b>Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals</b><br><br>The Read and Reward framework utilizes human-written instruction manuals to assist learning policies for specific tasks, leading to more efficient and better-performing agents in RL algorithms on Atari games.</span></a> utilizes human-written instruction manuals to improve RL algorithms on Atari games. Pre-training an agent on a set of tasks from the Meta-World benchmark suite and adapting it to tasks from Continual-World is feasible <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=75739e2a76a1426212805c9657823f9054ce7d25">[4]<span><b>Learning to Modulate pre-trained Models in RL</b><br><br>Pre-training an agent on a set of tasks from the Meta-World benchmark suite and adapting it to tasks from Continual-World is feasible, but existing fine-tuning methods often deteriorate performance on previously learned tasks, thus a novel approach is proposed to avoid forgetting.</span></a>, and the Extreme Q-Learning framework <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=2ead7a50553482fe540503dccde4908ba37dd56f">[5]<span><b>Extreme Q-Learning: MaxEnt RL without Entropy</b><br><br>Extreme Q-Learning framework using Extreme Value Theory (EVT) directly models maximal Q-value, avoiding out-of-distribution actions, and achieves strong performance.</span></a> achieves strong performance. Additionally, a new method for learning object interaction skills in entity-centric environments is introduced <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=5b4ec36c31a187be880b21e4b2c5555eeb8025aa">[6]<span><b>Unsupervised Object Interaction Learning with Counterfactual Dynamics Models</b><br><br>A new method for learning object interaction skills in entity-centric environments using a structured goal representation and counter-factual intrinsic reward.</span></a>, and MAESTRO <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=88f15b684b7ac94a7e76e9545ff67c4ae1037cb5">[7]<span><b>MAESTRO: Open-Ended Environment Design for Multi-Agent Reinforcement Learning</b><br><br>The study introduces MAESTRO, a multi-agent unsupervised environment design approach that generates adversarial curricula for two-player zero-sum settings, outperforming strong baselines.</span></a> generates adversarial curricula for two-player zero-sum settings. S4RL <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=8c80332463985c13899d2acdd8ee1e224cf633f8">[8]<span><b>Decision S4: Efficient Sequence-Based RL via State Spaces Layers</b><br><br>S4RL, a state-space layer-based model, outperforms decision transformers in off-policy reinforcement learning, with reduced latency, parameters, and training time.</span></a> outperforms decision transformers in off-policy reinforcement learning, and D4MORL and PEDA <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=c07ec7dac8f965c1806b09a6a43775a1172ae241">[9]<span><b>Scaling Pareto-Efficient Decision Making via Offline Multi-Objective RL</b><br><br>The paper proposes a new data-driven setup for offline multi-objective reinforcement learning, introducing D4MORL and Pareto-Efficient Decision Agents (PEDA).</span></a> introduce a new data-driven setup for offline multi-objective reinforcement learning. Lastly, a policy optimization method for competitive multi-agent reinforcement learning is proposed <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=fdc80296e39acfb9afbe7f59981625c46e870a3f">[10]<span><b>Faster Last-iterate Convergence of Policy Optimization in Zero-Sum Markov Games</b><br><br>This paper proposes a policy optimization method for competitive multi-agent reinforcement learning in two-player zero-sum Markov games, achieving finite-time last-iterate linear convergence.</span></a>.<br><br><br><h2>Robustness and Generalization in ML</h2><br>The papers in this collection present innovative approaches to improve the robustness and generalization of machine learning models, addressing challenges such as noisy labels, selective classification, and out-of-distribution detection.<br><br><br>TCM <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=1d046376c61293bca6c6f3f2fa8a4f8f08799c44">[1]<span><b>Conservative Prediction via Transductive Confidence Minimization</b><br><br>Transductive confidence minimization (TCM) is proposed to mitigate the challenge of selective classification and out-of-distribution detection in machine learning models for safety-critical settings.</span></a> proposes a transductive confidence minimization method to address selective classification and out-of-distribution detection in safety-critical settings. DORA <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=2377cd1dfd8b4cd176e4a6d5017ac7cf90d8f291">[2]<span><b>DORA: Exploring Outlier Representations in Deep Neural Networks</b><br><br>DORA is a data-agnostic framework that uses the Extreme-Activation distance measure to identify artifact representations in Deep Neural Networks.</span></a> introduces a data-agnostic framework to identify artifact representations in deep neural networks. EPL <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=33cb50028341718bbc0b6f631ca8b78dd7498943">[3]<span><b>EFFICIENT UTILIZATION OF PRE-TRAINED MODEL FOR LEARNING WITH NOISY LABELS</b><br><br>EPL is a new algorithm that utilizes pre-trained models to cleanse noisy labels and improve the robustness of machine learning models.</span></a> utilizes pre-trained models to cleanse noisy labels and improve robustness. ISP <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=8b8503fd5c22c8fc90fd9331fafa03eabc35c50e">[4]<span><b>Pushing the Accuracy-Group Robustness Frontier with Introspective Self-play</b><br><br>Introspective Self-play (ISP) is a simple approach that improves the accuracy-group robustness trade-off frontier of deep neural network models.</span></a> presents a simple approach to improve the accuracy-group robustness trade-off frontier of deep neural network models. TTAB <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=cfdc72dc3ff43dc4ebb6ccb8da277bdd3ab0d336">[5]<span><b>On Pitfalls of Test-Time Adaptation</b><br><br>A large-scale open-sourced Test-Time Adaptation Benchmark (TTAB) is presented to identify common pitfalls in prior efforts of Test-Time Adaptation (TTA).</span></a> is a large-scale benchmark to identify common pitfalls in prior efforts of Test-Time Adaptation. DynaMS <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=5a54d6e397ef7790ce279c7a6229f314614f371d">[6]<span><b>DynaMS: Dyanmic Margin Selection for Efficient Deep Learning</b><br><br>Dynamic margin selection (DynaMS) is proposed to construct a subset of informative samples for training deep learning models, resulting in better generalization.</span></a> constructs a subset of informative samples for training deep learning models, resulting in better generalization. PAPLUDA <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=5cbdc27f2acc6b82d95855e4d341418bc3320bfd">[7]<span><b>Partial Label Unsupervised Domain Adaptation with Class-Prototype Alignment</b><br><br>PAPLUDA is a novel Prototype Alignment based method that addresses the challenging partial label unsupervised domain adaptation problem, achieving state-of-the-art performance.</span></a> addresses the challenging partial label unsupervised domain adaptation problem, achieving state-of-the-art performance. Finally, TVSPrune <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=edd4fcf4dee5df43c99842dda8252b5213c2538f">[8]<span><b>TVSPrune - Pruning Non-discriminative filters via Total Variation separability of intermediate representations without fine tuning</b><br><br>The discriminative filters hypothesis proposes a new paradigm for pruning neural networks, called distributional pruning, which only requires access to the distributions that generated the original datasets, and the authors present a novel one-shot pruning algorithm, called TVSPrune, that identifies non-discriminative filters for pruning, achieving up to 60% parameter pruning with only a 2% loss of accuracy without fine-tuning.</span></a> proposes a new paradigm for pruning neural networks, achieving up to 60% parameter pruning with only a 2% loss of accuracy without fine-tuning.<br><br><br><h2>Natural Language Processing Innovations</h2><br>The latest research in natural language processing focuses on enhancing model stability, pre-training models for universal effectiveness, and developing multimodal agents for autonomous navigation.<br><br><br>The use of pseudo-label training (PLT) enhances the stability of over-parameterized deep neural models in neural machine translation (NMT) <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=2d572c3562f66ea11f12de57a5ed1f961a6e4b68">[1]<span><b>Pseudo-label Training and Model Inertia in Neural Machine Translation</b><br><br>Over-parameterized deep neural models in neural machine translation (NMT) can be brittle, but pseudo-label training (PLT) can enhance model stability to updates and input perturbations.</span></a>, while a unified framework for pre-training models achieves state-of-the-art (SOTA) performance on 50 NLP tasks <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=2db7fac67c690d80d8b68567e6032edd31a04661">[2]<span><b>UL2: Unifying Language Learning Paradigms</b><br><br>A unified framework for pre-training models that are universally effective across datasets and setups is proposed, achieving SOTA performance on 50 NLP tasks.</span></a>. Self-supervised learning models can be extracted from APIs through black-box query access <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=4615365bba1bda4cd045b864a6cc186a6fe4e4fb">[3]<span><b>Sentence Embedding Encoders are Easy to Steal but Hard to Defend</b><br><br>Self-supervised learning models can be efficiently extracted from APIs through black-box query access, highlighting the need for embedding secret downstream tasks to protect against stealing.</span></a>, and DeCap introduces a lightweight visual-aware language decoder for zero-shot captioning <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=800604d777a5e522dd1b1fd27bfdba1457e06121">[4]<span><b>DeCap: Decoding CLIP Latents for Zero-Shot Captioning via Text-Only Training</b><br><br>DeCap, a simple framework for zero-shot captioning, introduces a lightweight visual-aware language decoder that generates high-quality descriptions using text-only data.</span></a>. Continual domain-adaptive pre-training of language models improves end-task performance <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=95b15e6145029721347f9996f2773d1b77d6bb66">[5]<span><b>Continual Pre-training of Language Models</b><br><br>Continual domain-adaptive pre-training of language models using a soft-masking mechanism improves end-task performance and knowledge transfer.</span></a>, and a multimodal agent for autonomous web navigation outperforms previous approaches <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=9c48d6dea10e415f0f9ba5e11d8e77bca1a6f5b1">[6]<span><b>Instruction-Finetuned Foundation Models for Multimodal Web Navigation</b><br><br>Researchers propose a multimodal agent for autonomous web navigation that outperforms previous approaches by a significant margin, using supervised finetuning of vision and language foundation models.</span></a>. Meta learning and sample selection strategies achieve SOTA results on NER tasks for low-resource languages <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=ace20966de455608877702bdba6ddfcf3d4c5248">[7]<span><b>MetaXLR - Mixed Language Meta Representation Transformation for Low-resource Cross-lingual Learning based on Multi-Armed Bandit</b><br><br>An enhanced approach using meta learning and a sample selection strategy achieved state-of-the-art results on NER task for extremely low-resource languages.</span></a>, and large language models possess the capability of out-of-context meta-learning <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=c3233f783e071d58d17669ed4b17040c83622fbc">[8]<span><b>Out-of-context Meta-learning in Large Language Models</b><br><br>Large language models (LLMs) possess the surprising capability of out-of-context meta-learning, which allows them to internalize semantic content and apply it appropriately.</span></a>. Lastly, novel Transformer decoding algorithms improve program generation <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=ef753072577c9bf2ca217138d9dbc96c4c67844d">[9]<span><b>Planning with Large Language Models for Code Generation</b><br><br>A novel Transformer decoding algorithm, Planning-Guided Transformer Decoding (PG-TD), uses a planning algorithm to guide the Transformer to generate better programs.</span></a> and provide an explainable solution to Question Answering over Knowledge Graphs <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=f6b9ad972b9eb3722721e33d00e71bc97a773a66">[10]<span><b>Answering Questions Over Knowledge Graphs Using Logic Programming Along with Language Models</b><br><br>Large language models equipped with logical programming languages can provide an explainable solution to Question Answering over Knowledge Graphs.</span></a>.<br><br><br><h2>ML in Healthcare and Beyond</h2><br>The use of deep learning and machine learning techniques in healthcare and other fields is becoming increasingly popular, with new models and frameworks being developed to improve accuracy and interpretability.<br><br><br>TabRet <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=2d4b0a92f198ad1752416c926682865049fa3ffd">[1]<span><b>TabRet: Pre-training Transformer-based Tabular Models for Unseen Columns</b><br><br>TabRet is a pre-trainable Transformer-based model for tabular data that achieved the best AUC performance on four healthcare datasets.</span></a> is a pre-trainable Transformer-based model that achieved the best AUC performance on four healthcare datasets, while HealthSyn <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=56361c484659a34e2201fb3e67a0ee11521f1e80">[2]<span><b>Synthetic Data Generator for Adaptive Interventions in Global Health</b><br><br>HealthSyn is an open-source synthetic data generator that uses Markov processes to test reinforcement learning algorithms in mobile health interventions.</span></a> is an open-source synthetic data generator that uses Markov processes to test reinforcement learning algorithms in mobile health interventions. SPIN <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=d71626e5a23dacdb7ae914f2581083129cde22d5">[3]<span><b>Semi-Parametric Inducing Point Networks and Neural Processes</b><br><br>SPIN is a semi-parametric inducing point network architecture that supports large contexts in meta-learning and improves accuracy.</span></a> is a semi-parametric inducing point network architecture that supports large contexts in meta-learning, and a pseudo-dynamic machine learning framework <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=a046912248e279f2beb1ee92ab9fb0d65d082f80">[4]<span><b>XMI-ICU: Explainable Machine Learning Model for Pseudo-Dynamic Prediction of Mortality and Heart Attack in the ICU</b><br><br>A pseudo-dynamic machine learning framework was developed to predict mortality and recurrent heart attack in ICU patients with interpretability and clinical risk analysis.</span></a> was developed to predict mortality and recurrent heart attack in ICU patients with interpretability and clinical risk analysis. STAGCN <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=58d5cd19db2f2aee4ac1da5c99a283b128a565d2">[5]<span><b>STAGCN: Spatial-Temporal Attention Based Graph Convolutional Networks for COVID-19 Forecasting</b><br><br>A novel deep learning framework, STAGCN, is proposed to forecast COVID-19 infections by effectively using spatial and temporal relationships.</span></a> is a novel deep learning framework that forecasts COVID-19 infections by effectively using spatial and temporal relationships. MRLA <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=4582d6e2b91a9b5bf7ec1a132072858a0c900a18">[6]<span><b>Cross-Layer Retrospective Retrieving via Layer Attention</b><br><br>A cross-layer attention mechanism called MRLA is proposed to enhance the representation power of deep neural networks in vision tasks.</span></a> is a cross-layer attention mechanism that enhances the representation power of deep neural networks in vision tasks. The 3D equivariant diffusion model <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=6ddb43b9dd2144403074eeb247a965bfc2a5cf18">[7]<span><b>3D Equivariant Diffusion for Target-Aware Molecule Generation and Affinity Prediction</b><br><br>A 3D equivariant diffusion model is developed to design drugs for a specific protein target, which generates more realistic 3D structures and better affinities.</span></a> is developed to design drugs for a specific protein target, while SGConv <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=dcc4b178c7092b07553268bd214e447ce7b34725">[8]<span><b>What Makes Convolutional Models Great on Long Sequence Modeling?</b><br><br>The paper proposes a new convolutional model called Structured Global Convolution (SGConv) that efficiently handles long-range dependencies and outperforms previous state-of-the-art models on several tasks.</span></a> is a new convolutional model that efficiently handles long-range dependencies and outperforms previous state-of-the-art models on several tasks. PhyloTransformer <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=aea86f5f1280e589715a10268e14a212c4674f7f">[9]<span><b>PhyloTransformer: A Self-supervised Discriminative Model for SARS-CoV-2 Viral Mutation Prediction Based on a Multi-head Self-attention Mechanism</b><br><br>PhyloTransformer, a self-supervised discriminative model, accurately models genetic mutations in SARS-CoV-2 sequences, identifying potential mutations for effective targeting of future variants.</span></a> accurately models genetic mutations in SARS-CoV-2 sequences, identifying potential mutations for effective targeting of future variants, and Recurrence-extended Transformer <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=defe67a0fc9c46c357863056f4a245ca6126b583">[10]<span><b>Learning to Solve Constraint Satisfaction Problems with Recurrent Transformer</b><br><br>Recurrence-extended Transformer is a promising approach to end-to-end learning for Constraint Satisfaction Problems, outperforming Graph Neural Networks and neuro-symbolic models.</span></a> is a promising approach to end-to-end learning for Constraint Satisfaction Problems, outperforming Graph Neural Networks and neuro-symbolic models.<br><br><br><h2>Self-Supervised Learning for Computer Vision</h2><br>The papers in this collection showcase the potential of self-supervised learning and data augmentation techniques to improve the performance of computer vision models in various applications.<br><br><br>Contrastive corpus similarity <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=f8967074a5bd7020f62fe9de26f34e4d9ea5215e">[1]<span><b>Contrastive Corpus Attribution for Explaining Representations</b><br><br>The proposed contrastive corpus similarity method generates semantically meaningful scalar explanations for unsupervised models, enabling insights and zero-shot object localization.</span></a> generates semantically meaningful scalar explanations for unsupervised models, while label propagation algorithms <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=7e0672f9e7d8bbeb8ed49b067f56d4e101f1758a">[2]<span><b>Label Propagation with Weak Supervision</b><br><br>A novel analysis of label propagation algorithm (LPA) is introduced, which exploits probabilistic hypothesized labels on unlabeled data, and incorporates multiple sources of noisy information, showing improvements on weakly supervised classification tasks.</span></a> improve weakly supervised classification tasks. Autonomous driving is addressed with a new multimodal dataset <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=102acc50c64aeb770498a54bfa6e74c48ee3049d">[3]<span><b>aiMotive Dataset: A Multimodal Dataset for Robust Autonomous Driving with Long-Range Perception</b><br><br>A new multimodal dataset for robust autonomous driving with long-range perception is introduced, consisting of 176 scenes with synchronized LiDAR, camera, and radar sensors.</span></a> and a self-supervised method <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=eb18e6002bf223b0bb1dff146f31c88cc68f9dcf">[4]<span><b>MaskedFusion: Reconstruct LiDAR data by Querying Camera Features</b><br><br>A novel self-supervised method is introduced to fuse LiDAR and camera data for self-driving applications, using masked autoencoders and dense spherical LiDAR projections.</span></a> that fuses LiDAR and camera data. Self-supervised learning <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=13853ef93a5f2f1c0fbd1afe06b061eaf1a84823">[5]<span><b>Self-supervised Learning to Predict Ejection Fraction using Motion-mode Images</b><br><br>Self-supervised learning can help overcome data scarcity in healthcare applications by learning meaningful representations from unlabeled data.</span></a><a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=1dff78d97105a8270eff207aa9b9606236bbb3bb">[6]<span><b>Understanding The Robustness of Self-supervised Learning Through Topic Modeling</b><br><br>Self-supervised learning can be oblivious to specific models, making it less susceptible to model misspecification, and can recover useful posterior information for general topic models.</span></a> can help overcome data scarcity in healthcare applications and is less susceptible to model misspecification. Probabilistic modeling <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=27a5ebcfb4674af70653169e2cde7d9df0dc0d55">[7]<span><b>Variational prompt tuning improves generalization of vision-language foundation models</b><br><br>A probabilistic modeling approach to prompt tuning of large vision-language models improves generalization capabilities for downstream tasks.</span></a> improves generalization capabilities for downstream tasks, and Visual Token Matching <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=2bdee56f237898273c49ffd59513ac1dc47d795d">[8]<span><b>Universal Few-shot Learning of Dense Prediction Tasks with Visual Token Matching</b><br><br>Visual Token Matching (VTM) is a universal few-shot learner for arbitrary dense prediction tasks that flexibly adapts to any task with a tiny amount of task-specific parameters.</span></a> is a universal few-shot learner. Finally, progressive mix-up <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=54096d91d6a67cc167f316fde0d159b3edaf6f85">[9]<span><b>Progressive Mix-Up for Few-Shot Supervised Multi-Source Domain Transfer</b><br><br>The paper proposes a new progressive mix-up mechanism to transfer knowledge from multiple source domains to a single target domain with few-shot data.</span></a> transfers knowledge from multiple source domains, and MosRep <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=5eace66a11ea363e965653304f21ad79cd112831">[10]<span><b>Mosaic Representation Learning for Self-supervised Visual Pre-training</b><br><br>The MosRep framework proposes a new data augmentation strategy that enriches the backgrounds of small crops, improving visual representations.</span></a> enriches the backgrounds of small crops to improve visual representations.<br><br><br><h2>Optimization Techniques for ML Models</h2><br>The papers in this collection explore various optimization techniques for machine learning models, including kernel bandit learning, Bayesian inference, and zeroth-order optimization, with a focus on improving robustness, communication efficiency, and stability.<br><br><br>Online SGD with weight decay <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=31ef10b0d82197012874d92618be35699065046d">[1]<span><b>Neural Networks Efficiently Learn Low-Dimensional Representations with SGD</b><br><br>Online SGD with weight decay can train a two-layer neural network to converge to the k-dimensional principal subspace, leading to a generalization error bound of O(sqrt(kd/T)), and allowing SGD-trained ReLU NNs to learn a single-index target with a sample complexity linear in d.</span></a> can train a two-layer neural network to converge to the k-dimensional principal subspace, while MORBiT <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=45c00944e0ae7ef63f88c57eb3abd7337982cc5b">[2]<span><b>Min-Max Multi-objective Bilevel Optimization with Applications in Robust Machine Learning</b><br><br>MORBiT, a novel single-loop gradient descent-ascent bilevel optimization algorithm, is designed to solve a generic min-max multi-objective bilevel optimization problem.</span></a> is a novel algorithm for solving min-max multi-objective bilevel optimization problems. Asynchronous distributed kernel bandit learning <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=b03f4454d2adbc866a8baf54595f93b262970658">[3]<span><b>Learning Kernelized Contextual Bandits in a Distributed and Asynchronous Environment</b><br><br>This paper proposes an asynchronous solution for distributed kernel bandit learning, based on approximated kernel regression, which improves robustness and communication efficiency.</span></a> improves robustness and communication efficiency, and the OT-profile <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=e1452d7eae10c11e6117d07c3aef56f1869d90d2">[4]<span><b>Computing all Optimal Partial Transports</b><br><br>The paper presents a novel framework to analyze the properties of the OT-profile and an algorithm to compute it, which is a piecewise-linear non-decreasing convex function, and can be used to improve prediction accuracy for outlier detection and PU-Learning experiments.</span></a> can be used to improve prediction accuracy for outlier detection and PU-Learning experiments. Understanding the impact of variance and bias in gradient estimators <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=bd0af88287744affc0f23c775926c37cdd141900">[5]<span><b>How gradient estimator variance and bias impact learning in neural networks</b><br><br>Understanding how variance and bias in gradient estimators impact learning dependent on network and task properties is crucial for effective learning.</span></a> is crucial for effective learning, and scalable sample-based Bayesian inference <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=c3b9310b8da36064f818daa6c0f4332bd3a30364">[6]<span><b>Sampling-based inference for large linear models, with application to linearised Laplace</b><br><br>A scalable sample-based Bayesian inference method for conjugate Gaussian multi-output linear models is introduced, allowing linearised neural network inference on large datasets.</span></a> allows linearised neural network inference on large datasets. Residual networks <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=ae67f0cc885bd798eb6805ab26c191087f371eae">[7]<span><b>A Kernel Perspective of Skip Connections in Convolutional Networks</b><br><br>Residual networks (ResNets) with ReLU activation maintain a similar frequency bias and have more favorable condition numbers, enabling faster convergence of training.</span></a> enable faster convergence of training, and ZoRD <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=e9570215e36febb507fa434f592249cd070bf2ba">[8]<span><b>Zeroth-Order Optimization with Trajectory-Informed Derivative Estimation</b><br><br>The proposed ZoRD algorithm for zeroth-order optimization uses trajectory-informed derivative estimation and dynamic virtual updates to improve query efficiency.</span></a> improves query efficiency for zeroth-order optimization.<br><br><br><h2>Novel Methods in Computer Vision</h2><br>Recent advances in computer vision and image processing have led to the development of novel methods for image generation, reconstruction, and manipulation, including the use of deep learning models and contrastive representation learning.<br><br><br>NIA <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=12c010f7b2c02a6b4da06e8077bfc8ad0e205960">[1]<span><b>Neural Image-based Avatars: Generalizable Radiance Fields for Human Avatar Modeling</b><br><br>The Neural Image-based Avatars (NIA) method synthesizes novel views and poses of human performers from sparse multi-view images, outperforming existing methods.</span></a> synthesizes novel views and poses of human performers, while VAEs <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=58c91c5bcc4f235d78b6e3cc2e71a1a73a336f2a">[2]<span><b>Explicitly Minimizing the Blur Error of Variational Autoencoders</b><br><br>A new formulation of the reconstruction term for Variational Autoencoders (VAEs) penalizes blurry image generation while maximizing ELBO.</span></a> penalize blurry image generation. The manifold hypothesis <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=6abc43f3541e70ee86d7a4a4c7b050c5911da66c">[3]<span><b>Verifying the Union of Manifolds Hypothesis for Image Data</b><br><br>The manifold hypothesis does not capture the low-dimensional structure of image data, and the union of manifolds hypothesis improves deep learning performance.</span></a> is challenged, and contrastive representation learning <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=23214cf5441fa79315407534cb1b96ea27635f77">[4]<span><b>Guiding Energy-based Models via Contrastive Latent Variables</b><br><br>A novel framework using contrastive representation learning improves and accelerates training of energy-based models, achieving lower FID scores with faster training.</span></a> improves energy-based models. Autoregressive deployment of CNPs <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=817b8e11c44beec013478d3a9f4ddca05b87b6d5">[5]<span><b>Autoregressive Conditional Neural Processes</b><br><br>Autoregressive deployment of Conditional Neural Processes (CNPs) allows factorised Gaussian CNPs to model highly dependent, non-Gaussian predictive distributions.</span></a> models highly dependent, non-Gaussian predictive distributions, and rectified flow <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=c617e06f712dbc9d4d31758fffdfdd136896c897">[6]<span><b>Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow</b><br><br>Rectified flow is a simple approach to learning neural ordinary differential equation models that provides a unified solution to generative modeling and domain transfer.</span></a> provides a unified solution to generative modeling and domain transfer. A differentiable hypergeometric distribution <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=83f912e82a74f64f0c2628a8735ef6e83666e01a">[7]<span><b>Learning Group Importance using the Differentiable Hypergeometric Distribution</b><br><br>The paper proposes a differentiable hypergeometric distribution to learn the size of subsets in weakly-supervised learning and clustering applications.</span></a> is proposed for weakly-supervised learning and clustering applications. Incorporating linguistic structures <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=4780416a72238c732d4b341cbbfd734d3d11220c">[8]<span><b>Training-Free Structured Diffusion Guidance for Compositional Text-to-Image Synthesis</b><br><br>Linguistic structures are incorporated into text-to-image synthesis models to improve attribute-binding and compositional skills, resulting in more accurate image compositions.</span></a> improves text-to-image synthesis, and SysBinder <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=f9928d4d3d98a11d32aaa1e512b5059ba580343e">[9]<span><b>Neural Systematic Binder</b><br><br>A neural mechanism called SysBinder is proposed to construct a structured representation called Block-Slot Representation for unstructured modalities.</span></a> constructs a structured representation for unstructured modalities.<br><br><br><h2>ML for Complex Physical Systems</h2><br>The use of machine learning and deep neural networks is revolutionizing the way we model and predict complex physical systems, from fluid dynamics to quantum error correction, while achieving state-of-the-art accuracy and efficiency.<br><br><br>Hypernetwork-based latent dynamical models <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=8526fbea2f17fe56cc1f3a2057750db3da428413">[1]<span><b>Evolve Smoothly, Fit Consistently: Learning Smooth Latent Dynamics For Advection-Dominated Systems</b><br><br>A data-driven, space-time continuous framework using hypernetwork-based latent dynamical models produces highly efficient surrogate models for complex physical systems.</span></a> and data-driven energy-based models <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=95528e007898b7d13ca2e00c226996d495c82ef4">[4]<span><b>Improved Training of Physics-Informed Neural Networks Using Energy-Based Priors: a Study on Electrical Impedance Tomography</b><br><br>A Bayesian approach using data-driven energy-based model as a prior is proposed to improve the accuracy and quality of tomographic reconstruction in electrical impedance tomography using physics-informed neural networks.</span></a> are used to improve the accuracy and efficiency of surrogate models for physical systems. In the field of fluid dynamics, a differentiable vortex particle method <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=1cc3094c8176be94d33746e1c9e91ed7cd4f1d12">[2]<span><b>Learning Vortex Dynamics for Fluid Inference and Prediction</b><br><br>A novel differentiable vortex particle method is proposed to infer and predict fluid dynamics from a single video, enabling the inference of hidden physics quantities and future prediction.</span></a> is proposed to infer and predict fluid dynamics from a single video, while DDPMs <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=2970fe1d78b95ba7db7fe9b56edce744cea5e808">[3]<span><b>Denoising Diffusion Probabilistic Models to Predict the Number Density of Molecular Clouds in Astronomy</b><br><br>DDPMs, a generative approach in Machine Learning and Computer Vision, are applied to infer the volume or number density of GMCs in astronomy, achieving an order of magnitude improvement in accuracy.</span></a> are used to infer the volume or number density of GMCs in astronomy. Other papers in this collection explore machine learning analogs of "mechanical intelligence" <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=45e6ebb87a8935bd6023e5af612da2f4cb5ac431">[5]<span><b>Neuromechanical Autoencoders: Learning to Couple Elastic and Neural Network Nonlinearity</b><br><br>Researchers develop machine learning analogs of mechanical intelligence by jointly learning the morphology of nonlinear elastic solids and a deep neural network to control it.</span></a>, data-efficient neural decoders <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=71596ef51e5d4c05151fcdbc0de07a4c39dcfc07">[6]<span><b>The END: An Equivariant Neural Decoder for Quantum Error Correction</b><br><br>Researchers introduce a data-efficient neural decoder that exploits symmetries of the problem, achieving state-of-the-art accuracy for quantum error correction.</span></a>, and symbolic regression models applied to financial economics equations <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=9e981c0f35e9e6c37580cf2346a909357a40a060">[7]<span><b>Symbolic Regressions in Non-Physical Systems</b><br><br>Symbolic regression models are applied to non-physical systems, specifically financial economics equations, and tested with deep learning and genetic programming.</span></a>.<br><br><br><h2>Graph Neural Networks Advancements</h2><br>Graph neural networks are becoming increasingly popular in machine learning, with researchers proposing new algorithms and frameworks to improve their performance and efficiency. These include automating circuit topology generation, improving graph dictionary learning, and tackling data quality issues. Additionally, GNNs have been applied to solve combinatorial optimization problems and linear programs, and researchers are exploring their limitations in expressing general MILPs. Finally, new frameworks are being developed to perform edge and node representation learning and provide self-explanation for message aggregations.<br><br><br>A neural network architecture <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=173d99c8d9ec59b01cd4851e97f3d0929fcdec01">[1]<span><b>Learning to Induce Causal Structure </b><br><br>A neural network architecture was designed to learn the mapping from observational and interventional data to graph structures, achieving state-of-the-art performance.</span></a> has achieved state-of-the-art performance in learning the mapping from observational and interventional data to graph structures. Circuit Graph Neural Network (CktGNN) <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=f90191ff39da6a480273243b64fd3418c0d10aa3">[2]<span><b>CktGNN:  Circuit Graph Neural Network for Electronic Design Automation</b><br><br>Circuit Graph Neural Network (CktGNN) automates circuit topology generation and device sizing, improving efficiency and introducing Open Circuit Benchmark (OCB).</span></a> automates circuit topology generation and device sizing, while an improved graph dictionary learning algorithm <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=8df96e8f108bbdfe4a8c3cac01add72c9368c01f">[3]<span><b>Robust Graph Dictionary Learning</b><br><br>An improved graph dictionary learning algorithm based on a robust Gromov-Wasserstein discrepancy is proposed to learn atoms from noisy graph data.</span></a> based on a robust Gromov-Wasserstein discrepancy is proposed to learn atoms from noisy graph data. GTrans <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=656ed59b6924b05392e6ab43ad29a1129510ddd3">[4]<span><b>Empowering Graph Representation Learning with Test-Time Graph Transformation</b><br><br>GTrans, a graph transformation framework, is proposed to tackle data quality issues in graph neural networks, achieving better performance than model adaptation.</span></a>, a graph transformation framework, tackles data quality issues in GNNs, and an actor-critic framework <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=671536f9fd0b8ec0a7af7f6798cf8068ea349d61">[5]<span><b>Graph-based Deterministic Policy Gradient for Repetitive Combinatorial Optimization Problems</b><br><br>An actor-critic framework for graph-based machine learning pipelines with non-differentiable blocks is proposed and applied to repetitive combinatorial optimization problems.</span></a> is proposed for graph-based machine learning pipelines with non-differentiable blocks. While deep learning and GNNs can accelerate MILP solution processes <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=8ad1f1137b1191a57d3f38a889f483b0c9ad5965">[6]<span><b>On Representing Mixed-Integer Linear Programs by Graph Neural Networks</b><br><br>Researchers have discovered that while deep learning and graph neural networks can accelerate MILP solution processes, they have limitations in expressing general MILPs.</span></a>, they have limitations in expressing general MILPs. Formal verification of output reachability in Message Passing Neural Networks is impossible <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=8c17cf3a5198349ba689000c6af003e536645f34">[7]<span><b>Fundamental Limits in Formal Verification of Message-Passing Neural Networks</b><br><br>Formal verification of output reachability in Message Passing Neural Networks is impossible, but possible for node-classifier MPNN with limited input graph degree.</span></a>, but possible for node-classifier MPNN with limited input graph degree. GNNs can reliably predict feasibility, boundedness, and optimal solutions for linear programs <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=a66d7450293bf5913f02846c89fa0d00bb02917f">[8]<span><b>On Representing Linear Programs by Graph Neural Networks</b><br><br>Graph neural networks (GNNs) can reliably predict feasibility, boundedness, and optimal solutions for linear programs (LPs) using machine learning.</span></a>, and Edgeformers <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=d574c85c2f48318bd439f3c182348230d3a90a72">[9]<span><b>Edgeformers: Graph-Empowered Transformers for Representation Learning on Textual-Edge Networks</b><br><br>Edgeformers is a framework that utilizes contextualized text semantics on edges to perform edge and node representation learning, outperforming state-of-the-art baselines.</span></a> outperforms state-of-the-art baselines by utilizing contextualized text semantics on edges to perform edge and node representation learning. Lastly, MSInterpreter <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=d675204263435c45623a8e21c6f72ff940e7da03">[10]<span><b>Message-passing Selection: Towards Interpretable GNNs for Graph Classification</b><br><br>MSInterpreter is an interpretable GNNs' inference paradigm that selects critical paths for message aggregations, providing self-explanation instead of post-hoc explanations.</span></a> provides self-explanation instead of post-hoc explanations by selecting critical paths for message aggregations in interpretable GNNs' inference paradigms.<br><br><br><h2>Federated Learning Challenges and Solutions</h2><br>Federated learning is a promising approach to train machine learning models on decentralized data, but it poses challenges to privacy and utility. Recent research proposes solutions such as personalized models, expert opinions, and reinforcement learning to improve performance and privacy.<br><br><br>\DPFEDREP\ <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=07c799989215d9706c8c17ab652ee248f5e25fae">[1]<span><b>Share Your Representation Only: Guaranteed Improvement of the Privacy-Utility Tradeoff in Federated Learning</b><br><br>Federated learning's repeated parameter sharing causes significant information leakage, but a new algorithm called \DPFEDREP\ improves utility-privacy trade-off by a factor of $\sqrt{d}$.</span></a> improves the utility-privacy trade-off, while a generic bandit algorithm <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=13ee1b1a673e2db3f4e4b2c62fd3c8b70e87f8fb">[2]<span><b>Distributed Differential Privacy in Multi-Armed Bandits</b><br><br>A generic bandit algorithm based on successive arm elimination is designed to obtain a pure-DP guarantee under distributed trust model.</span></a> obtains a pure-DP guarantee under a distributed trust model. FRESCO <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=1bd610d57ee568b48decaad792d9082a732e9473">[3]<span><b>FRESCO: Federated Reinforcement Energy System for Cooperative Optimization</b><br><br>FRESCO is a framework that uses reinforcement learning agents trained with federated learning to implement energy markets and create a cooperative energy grid.</span></a> uses reinforcement learning agents to implement energy markets, and FedRank <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=20f9aa0bdda3589074e0ee46fb7eabeffea6b356">[4]<span><b>Learn to Select: Efficient Cross-device Federated Learning via Reinforcement Learning</b><br><br>FedRank is a novel federated learning approach that uses reinforcement and imitation learning to address device selection problems, improving model accuracy and training speed.</span></a> uses reinforcement and imitation learning to address device selection problems. Membership inference attacks <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=341bdab36cade0a7180ce115ad12470905fe66b3">[5]<span><b>Effective passive membership inference attacks in federated learning against overparameterized models</b><br><br>The paper discusses the challenge of performing membership inference attacks in a federated learning setting for image classification using passive white-box attacks.</span></a> in federated learning for image classification are discussed, and a new family of regularized Rényi divergences <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=3fbb9f44b9bccdafa2a27f07abfa8286431f8f67">[6]<span><b>Function-space regularized Rényi divergences</b><br><br>A new family of regularized Rényi divergences is proposed, which is well-behaved when $\alpha>1$, exhibits lower variance, and inherits features from IPMs.</span></a> is proposed. Few-shot transfer learning <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=9e610e12b94de49e43b02e335315531eabe4efb0">[7]<span><b>Differentially Private Federated Few-shot Image Classification</b><br><br>Few-shot transfer learning can be an effective approach to achieve user-level differentially private federated learning with state-of-the-art performance.</span></a> achieves user-level differentially private federated learning, and personalized federated learning <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=a1078e26b42924a8918493fe621a6086175c5944">[8]<span><b>When will federated learning change from generalization to personalization?</b><br><br>Personalized federated learning can improve performance in heterogeneous situations by building a personalized model for each client, improving both generalization and personalization.</span></a> improves performance in heterogeneous situations. Incorporating expert opinions <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=292ea945807e02f1c1d9f9b67dd8bc902d228770">[9]<span><b>Dynamic Human AI Collaboration</b><br><br>Incorporating expert opinions in machine learning models can improve accuracy and relevance, and a Bayesian framework with information sharing enhances performance.</span></a> enhances performance, and federated learning is feasible for training a variational autoencoder <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=be0491d1dc84fd91bbea7e79538c377dab70679c">[10]<span><b>Federated Learning with Variational Autoencoders</b><br><br>Federated learning is feasible for training a variational autoencoder to generate handwritten digits with comparable results to centralized models.</span></a>.<br><br><br><h2>Robustness Against Adversarial Attacks</h2><br>The papers in this collection explore various methods to improve the robustness of machine learning models against adversarial attacks, including novel activation functions, architecture search, and certification frameworks.<br><br><br>One paper proposes a new method for computing an appropriate reference for path integration schemes, improving the robustness of integral-based attribution methods <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=3c160b32df00923b38da9e120f23143fa19a0a40">[1]<span><b>Re-calibrating Feature Attributions for Model Interpretation</b><br><br>A new method for computing an appropriate reference for path integration scheme improves the sensitivity, sanity preservation, and model robustness of integral-based attribution methods.</span></a>. Another paper fine-tunes computer vision models to align with biological representations of primate object recognition behavior, leading to more robust and human-like behavior <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=8270824719e9931299104a6777ffd1678fbdb957">[2]<span><b>Aligning Model and Macaque Inferior Temporal Cortex Representations Improves Model-to-Human Behavioral Alignment and Adversarial Robustness</b><br><br>Fine-tuning computer vision models to align with biological representations of primate object recognition behavior leads to more robust and human-like behavior.</span></a>. A novel method for creating adversarial attacks in a black box environment is proposed using a single encoder and a three-encoder Transformer based GAN <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=6d9b4b01b310c074cef9a9f29ab2d13a1839b60e">[3]<span><b>Using vision transformer-based GANs against Vision Transformers</b><br><br>A novel method for creating adversarial attacks in a black box environment using a single encoder and a three-encoder Transformer based GAN.</span></a>. TextShield proposes a saliency-based detector and corrector to effectively detect and convert adversarial sentences to benign ones <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=8d364d243ebb57299d4e138b43a2632a189fcf2c">[4]<span><b>TextShield: Beyond Successfully Detecting Adversarial Sentences in text classification</b><br><br>TextShield proposes a saliency-based detector and corrector to effectively detect and convert adversarial sentences to benign ones, outperforming existing detectors.</span></a>. Ensemble of single hidden layer sign activation 01 loss networks are more robust to adversarial attacks than its differentiable counterparts <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=509f8d0e71428b4ba1b780be7eec90ce94fa6a55">[5]<span><b>Accuracy of white box and black box adversarial attacks on a sign activation 01 loss neural network ensemble</b><br><br>Ensemble of single hidden layer sign activation 01 loss networks are more robust to adversarial attacks than its differentiable counterparts.</span></a>. Learnable Logmoid Activation Unit (LAU) is proposed as a novel activation function for deep neural networks, outperforming well-known functions for various tasks <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=bbca11d15aa0ce76f5ba4e98ff3feb308d6aa063">[6]<span><b>A two-parameter learnable Logmoid Activation Unit</b><br><br>Learnable Logmoid Activation Unit (LAU) proposed as a novel activation function for deep neural networks, outperforming well-known functions for various tasks.</span></a>. Robust Neural Architecture Search by Cross-Layer Knowledge Distillation (RNAS-CL) is a novel NAS algorithm that improves the robustness of deep neural networks against adversarial attacks <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=feea3282e9eb7d3e88d769a1cc27e790ef4cacdd">[7]<span><b>Robust Neural Architecture Search by Cross-Layer Knowledge Distillation</b><br><br>Robust Neural Architecture Search by Cross-Layer Knowledge Distillation (RNAS-CL) is a novel NAS algorithm that improves the robustness of deep neural networks against adversarial attacks.</span></a>. Researchers propose a novel bilevel optimization problem to construct strong poison examples that maximize the attack success rate of the trained model in backdoor attacks <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=5b71969abb86950619d54a2d155bcb945b0e0879">[8]<span><b>Few-shot Backdoor Attacks via Neural Tangent Kernels</b><br><br>Researchers propose a novel bilevel optimization problem to construct strong poison examples that maximize the attack success rate of the trained model in backdoor attacks.</span></a>. The paper presents a framework that certifies robustness of machine learning models against natural transformations of images, including non-adversarial perturbations <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=9c7fbe0b429052c9f70c69d18edaca8548d7f89c">[9]<span><b>Provable Robustness against Wasserstein Distribution Shifts via Input Randomization</b><br><br>The paper presents a framework that certifies robustness of machine learning models against natural transformations of images, including non-adversarial perturbations.</span></a>. Lastly, the study explores the relationship between network architecture, robustness, filtering mechanisms, and representational straightness in response to time-varying input <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=e3f827bfbc13f16432819c8f057cc8ef38dd0e60">[10]<span><b>Exploring perceptual straightness in learned visual representations</b><br><br>The study explores the relationship between network architecture, robustness, filtering mechanisms, and representational straightness in response to time-varying input.</span></a>.<br><br><br><h2>Novel Approaches to Improve ML Performance</h2><br>The papers explore novel approaches to improve machine learning performance, including regularization, continual learning, and population structure modeling, while also considering biological plausibility and interpretability.<br><br><br>The forward-forward algorithm is evaluated in imitation learning <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=9706b00ee64a9e33a5f9da43ecea22ac67fb1cf0">[1]<span><b>IMITATION LEARNING USING THE FORWARD-FORWARD ALGORITHM</b><br><br>The study evaluates the forward-forward algorithm in the context of imitation learning, showing comparable performance to backpropagation on larger datasets.</span></a>, while TANGOS <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=04265b62be1415b20f56fb87c64fc8e5e07b44eb">[2]<span><b>TANGOS: Regularizing Tabular Neural Networks through Gradient Orthogonalization and Specialization</b><br><br>TANGOS is a novel framework for regularization in the tabular setting built on latent unit attributions, which encourages neurons to focus on sparse, non-overlapping input features, resulting in improved out-of-sample generalization performance.</span></a> improves generalization performance through regularization. Compositional architectures <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=1e081cce63fc7019c0fcb21218fa90a33343f6d3">[3]<span><b>On The Specialization of Neural Modules</b><br><br>Compositional architectures in machine learning aim to learn specialized modules to solve novel problems, but module specialization is not guaranteed.</span></a> aim to learn specialized modules, and a biologically plausible neural network <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=7e7d4391623c01fe6af2b914c9d928bc084394a2">[4]<span><b>Correlative Information Maximization Based Biologically Plausible Neural Networks for Correlated Source Separation</b><br><br>A biologically plausible neural network that extracts correlated latent sources by exploiting information about their domains is proposed.</span></a> extracts correlated latent sources. A modified MLP <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=bf6f4414ec448e0ac146f9483bda86388d6f31c2">[5]<span><b>Sparse Distributed Memory is a Continual Learner</b><br><br>A modified Multi-Layered Perceptron (MLP) that is a strong continual learner is created by connecting Sparse Distributed Memory (SDM) with the Transformer model.</span></a> connects SDM with the Transformer model to create a strong continual learner. TAME-GP <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=cad72af788188935a71bbff6ac4ba34033063c5e">[6]<span><b>A probabilistic framework for task-aligned intra- and inter-area neural manifold estimation</b><br><br>A novel probabilistic framework called TAME-GP allows for interpretable partitioning of population variability within and across areas in neural recordings during behavior.</span></a> partitions population variability in neural recordings, and a Bayesian network framework <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=1a3c742029897ba4ff28899698f5602fbad3f6e3">[7]<span><b>Efficient approximation of neural population structure and correlations with probabilistic circuits</b><br><br>A computationally efficient Bayesian network framework models population structures with high order correlations and large neurons, accurately approximating and quantifying neural correlations.</span></a> models population structures. A minimal computational model <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=4233d8b58e58856e137f20556af552c066db4673">[8]<span><b>Incremental Learning of Structured Memory via Closed-Loop Transcription</b><br><br>A minimal computational model for structured memory learning of multiple object classes in an incremental setting is proposed, which is simpler and more efficient than existing approaches.</span></a> for structured memory learning is proposed, and the properties of PCNs are analyzed <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=90ed450c6d0e015b61ad3dc7a932e804e2ae3535">[9]<span><b>A Theoretical Framework for Inference and Learning in Predictive Coding Networks</b><br><br>The paper provides a comprehensive theoretical analysis of the properties and dynamics of Predictive Coding Networks (PCNs) trained with prospective configuration.</span></a>. Finally, group and representation theory <a class="tip" target=”_blank” href="https://search.zeta-alpha.com/?doc_ids=c1df4a3b62ab98468a777709b75550b8ca4cddf7">[10]<span><b>Actionable Neural Representations: Grid Cells from Minimal Constraints</b><br><br>The brain must build actionable representations of variables in the external world to afford flexible behavior, which can be formulated using group and representation theory.</span></a> are used to formulate actionable representations of variables in the external world.<br><br> <br>ICLR 2023 promises to be an exciting and informative event for anyone interested in machine learning and related fields. With a wide range of topics and themes, the conference will provide a platform for sharing the latest research and developments, as well as discussing the challenges and opportunities that lie ahead. Whether you are a seasoned researcher or a newcomer to the field, ICLR 2023 is an event not to be missed.</div></body></html>